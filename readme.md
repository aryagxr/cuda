**CUDA Progress**

| **Day**    | **Code Summary**                                                   |
|------------|--------------------------------------------------------------------|
| Day 1      |  CUDA set up and kernel that prints "Hello World"                  |
| Day 2      |  CUDA kernel that adds two vectors                                 |
| Day 3      |  Adding matrices                                                   |
| Day 4      |  Vector addition using cuBLAS                                      |
| Day 5      |  Naive matmul                                                      |
| Day 6      |  Tiled matmul using shared memory                                  |
| Day 7      |  Naive 1D convolution with boundary checks                         |
| Day 8      |  Matrix multiplication using cuBLAS                                |
| Day 9      |  Matrix Transpose                                                  |
| Day 10 ðŸ¥³  |  Naive Softmax                                                     |
| Day 11     |  Softmax using shared memory and reductions                        |
| Day 12     |  Softmax using warp shuffle functions                              |
| Day 13     |  1D complex-to-complex fourier transform using cuFFT               |
| Day 14     |  Naive layer normalization                                         |
| Day 15     |  Optimizing layer norm using shared memory                         |
| Day 16     |  Optimizing layer norm using warp shuffle functions                |
| Day 17     |  Optimizing layer norm using vectorized loads                      |
| Day 18     |  Tiled 1D convolution and halo cells                               |
| Day 19     |  1D convolution using L2 cache                                     |
| Day 20 ðŸ¥³  |  [Blog Post: Optimizing Layer Normalization with CUDA](https://aryagxr.com/blogs/cuda-optimizing-layernorm) |
| Day 21     |  Simple self attention                                             |
| Day 22     |  Optimizing self attention                                         |
| Day 23     |  Causal attention with masking                                     |
| Day 24     |  Causal attention + torch binding                                  |
| Day 25     |  Multi-head attention                                              |
| Day 26     |  Parallel add using koggle stone algorithm                         |
| Day 27     |  MHA debug                                                         |
| Day 28     |  Flash Attention 1 (algorithm 1) Forward pass                      |
| Day 29     |  Flash Attention 1 (algorithm 1) Forward pass continued            |
| Day 30 ðŸ¥³  |  Flash Attention 1 (algorithm 1) Forward pass                      |
| Day 31     |  HGEMV matvec using fp16                                           |
| Day 32     |  HGEMV matvec using Bfloat16                                       |

